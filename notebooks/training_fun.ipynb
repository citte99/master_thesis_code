{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "Currently this dataloader is calculating the images in float32\n",
      "Currently not cropping and rotating, set final_transform in the intialization to True to use it\n",
      "\n",
      "[DEBUG] Original test_images shape: torch.Size([100, 1, 100, 100])\n",
      "[DEBUG] Original test_labels shape: torch.Size([100])\n",
      "[DEBUG] Test images already have a channel dimension.\n",
      "\n",
      "[DEBUG] Model architecture:\n",
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "[DEBUG] First conv layer weight shape: torch.Size([64, 1, 7, 7])\n",
      "\n",
      "[DEBUG] Attempting a single forward pass on the test batch...\n",
      "[DEBUG] Single forward pass outputs shape: torch.Size([100, 2])\n",
      "\n",
      "[DEBUG] Starting training...\n",
      "\n",
      "Currently not cropping and rotating, set final_transform in the intialization to True to use it\n",
      "[DEBUG] Epoch 1 first batch images shape: torch.Size([100, 1, 100, 100]), labels shape: torch.Size([100])\n",
      "Currently not cropping and rotating, set final_transform in the intialization to True to use it\n",
      "Currently not cropping and rotating, set final_transform in the intialization to True to use it\n",
      "Currently not cropping and rotating, set final_transform in the intialization to True to use it\n",
      "Currently not cropping and rotating, set final_transform in the intialization to True to use it\n",
      "Currently not cropping and rotating, set final_transform in the intialization to True to use it\n",
      "Currently not cropping and rotating, set final_transform in the intialization to True to use it\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[1;32m     77\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Ensure images have a channel dimension: [batch_size, 1, H, W]\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m images\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     82\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/master_thesis_code/src/deep_learning/NN_datasets/dataloaders.py:48\u001b[0m, in \u001b[0;36mcustom_dataloader.<locals>.<lambda>\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m     42\u001b[0m index_dataset \u001b[38;5;241m=\u001b[39m IndexDataset(\u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[1;32m     43\u001b[0m sampler \u001b[38;5;241m=\u001b[39m RandomBatchSampler(index_dataset, batch_size, shuffle)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(\n\u001b[1;32m     46\u001b[0m     index_dataset,\n\u001b[1;32m     47\u001b[0m     batch_sampler\u001b[38;5;241m=\u001b[39msampler,\n\u001b[0;32m---> 48\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m indices: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     50\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/master_thesis_code/src/deep_learning/NN_datasets/custom_datasets/no_noise_dataset.py:96\u001b[0m, in \u001b[0;36mNoNoiseDataset.get_batch\u001b[0;34m(self, idxs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     grid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muncropped_grid\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muncropped_grid\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muncropped_grid\u001b[38;5;241m.\u001b[39mrepeat(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     95\u001b[0m     lensing_system\u001b[38;5;241m=\u001b[39mLensingSystemBroadcasting(configs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 96\u001b[0m     images_batch\u001b[38;5;241m=\u001b[39m\u001b[43mlensing_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     labels_batch\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack(labels_batch)\n\u001b[1;32m    100\u001b[0m images_batch\u001b[38;5;241m=\u001b[39m images_batch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/master_thesis_code/src/lensing_system_broadcasting/lensing_system.py:84\u001b[0m, in \u001b[0;36mLensingSystem.forward\u001b[0;34m(self, lens_grid)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m lens_grid\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:  \u001b[38;5;66;03m# Only process within batch size\u001b[39;00m\n\u001b[1;32m     83\u001b[0m         system_source_grid \u001b[38;5;241m=\u001b[39m source_grid[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# [1, H, W, 2]\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m         output[i] \u001b[38;5;241m=\u001b[39m \u001b[43msource_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_source_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/master_thesis_code/src/lensing_system_broadcasting/sources/gaussian_blob.py:177\u001b[0m, in \u001b[0;36mGaussianBlob.forward\u001b[0;34m(self, source_grid)\u001b[0m\n\u001b[1;32m    174\u001b[0m I_expanded\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mI\u001b[38;5;241m.\u001b[39mexpand(batch_size)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# 3. Calculate rotated coordinates efficiently\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m xrel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimized_translate_rotate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient_expanded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# 4. Calculate squared distance (in-place)\u001b[39;00m\n\u001b[1;32m    181\u001b[0m rs2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrs2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/master_thesis_code/src/lensing_system_broadcasting/sources/gaussian_blob.py:142\u001b[0m, in \u001b[0;36mGaussianBlob._optimized_translate_rotate\u001b[0;34m(self, source_grid, position, orient_rad)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplex_z\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_complex(\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiff\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size, height, width, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# 3. Get rotation factor and multiply (in-place)\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m rot_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_rotation_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient_rad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m torch\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplex_z\u001b[39m\u001b[38;5;124m'\u001b[39m], rot_factor, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrotated_z\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# 4. Convert back to real coordinates\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/master_thesis_code/src/lensing_system_broadcasting/sources/gaussian_blob.py:125\u001b[0m, in \u001b[0;36mGaussianBlob._get_rotation_factor\u001b[0;34m(self, batch_size, orient_rad)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Compute rotation factor (in-place when possible)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m neg_i_theta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39mj \u001b[38;5;241m*\u001b[39m orient_rad\n\u001b[0;32m--> 125\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneg_i_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rotation_factors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rotation_factors[batch_size]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#from torch.utils.data import DataLoader\n",
    "#from deep_learning import ResNetDataset\n",
    "\n",
    "\n",
    "from deep_learning import custom_dataloader\n",
    "from deep_learning import NoNoiseDataset\n",
    "\n",
    "from deep_learning import ResNet50\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from shared_utils import _grid_lens\n",
    "\n",
    "# Choose the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "model = ResNet50(num_classes=2)\n",
    "model = model.float()  # Converts all weights and buffers to torch.float64\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Setup and Training ---\n",
    "grid_lens = _grid_lens(6.0, 100, device=device)\n",
    "\n",
    "\n",
    "\n",
    "# # Create an instance of your training dataset.\n",
    "# train_dataset=ResNetDataset(catalog_name=\"reposition_sources\", use_only_a_percent=50, mode=\"on_gpu_generation\", uncropped_grid=grid_lens)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=0)\n",
    "\n",
    "# Create dataset\n",
    "train_dataset= NoNoiseDataset(catalog_name=\"SIS_10e10_sub_train\", samples_used=2000, grid_width_arcsec=6.0, grid_pixel_side= 100, broadcasting=True)\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = custom_dataloader(train_dataset, batch_size=100, num_workers=0)\n",
    "\n",
    "\n",
    "# Debug: Check what one batch looks like\n",
    "test_images, test_labels = next(iter(train_loader))\n",
    "print(\"\\n[DEBUG] Original test_images shape:\", test_images.shape)\n",
    "print(\"[DEBUG] Original test_labels shape:\", test_labels.shape)\n",
    "\n",
    "# If the images don't have a channel dimension, add one.\n",
    "if test_images.ndim == 3:\n",
    "    test_images = test_images.unsqueeze(1)\n",
    "    print(\"[DEBUG] After unsqueeze, test_images shape:\", test_images.shape)\n",
    "else:\n",
    "    print(\"[DEBUG] Test images already have a channel dimension.\")\n",
    "\n",
    "\n",
    "print(\"\\n[DEBUG] Model architecture:\")\n",
    "print(model)\n",
    "print(\"\\n[DEBUG] First conv layer weight shape:\", model.conv1.weight.shape)\n",
    "\n",
    "# Test a single forward pass before training\n",
    "try:\n",
    "    print(\"\\n[DEBUG] Attempting a single forward pass on the test batch...\")\n",
    "    test_images = test_images.to(device)\n",
    "    test_labels = test_labels.to(device).long()\n",
    "    test_outputs = model(test_images)\n",
    "    print(\"[DEBUG] Single forward pass outputs shape:\", test_outputs.shape)\n",
    "except Exception as e:\n",
    "    print(\"[DEBUG] Error during single forward pass:\", e)\n",
    "    exit()\n",
    "\n",
    "# Set up loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "print(\"\\n[DEBUG] Starting training...\\n\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        # Ensure images have a channel dimension: [batch_size, 1, H, W]\n",
    "        if images.ndim == 3:\n",
    "            images = images.unsqueeze(1)\n",
    "        if batch_idx == 0:\n",
    "            print(f\"[DEBUG] Epoch {epoch+1} first batch images shape: {images.shape}, labels shape: {labels.shape}\")\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()       # Zero the gradients\n",
    "        outputs = model(images)     # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()             # Backward pass\n",
    "        optimizer.step()            # Update parameters\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"[DEBUG] Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# --- Saving the Model Parameters ---\n",
    "\n",
    "# Save the trained model's parameters to a file.\n",
    "model_save_path = 'resnet50_trained_parameters.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"\\n[DEBUG] Model parameters saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Currently this dataloader is calculating the images in float32\n",
      "\n",
      "[DEBUG] Accuracy on the test dataset: 85.15%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from deep_learning import custom_dataloader\n",
    "from deep_learning import NoNoiseDataset\n",
    "\n",
    "from deep_learning import ResNet50\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from shared_utils import _grid_lens\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "grid_lens = _grid_lens(6.0, 100, device=device)\n",
    "\n",
    "\n",
    "\n",
    "test_dataset= NoNoiseDataset(catalog_name=\"SIS_10e10_sub_val_extended\", samples_used=2000, grid_width_arcsec=6.0, grid_pixel_side= 100, broadcasting=False)\n",
    "\n",
    "# Create dataloader\n",
    "test_loader = custom_dataloader(test_dataset, batch_size=100, num_workers=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = ResNet50(num_classes=2)\n",
    "#load the saved parameters\n",
    "model.load_state_dict(torch.load('resnet50_trained_parameters.pth'))\n",
    "\n",
    "model.to(device)\n",
    "# Set the model to evaluation mode.\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# We don't need gradients for evaluation.\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Ensure images have the channel dimension.\n",
    "        if images.ndim == 3:\n",
    "            images = images.unsqueeze(1)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        # Get the predicted class with the highest score\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"\\n[DEBUG] Accuracy on the test dataset: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Currently this dataloader is calculating the images in float32\n",
      "\n",
      "[DEBUG] Time taken to load 10 batches: 1.4698 seconds\n",
      "Using device: cuda\n",
      "Currently this dataloader is calculating the images in float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/Desktop/master_thesis_code/src/deep_learning/NN_datasets/custom_datasets/no_noise_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_tensor = torch.tensor(num_sub > 0, device=self.device).long()\n",
      "/home/francesco/Desktop/master_thesis_code/src/deep_learning/NN_datasets/custom_datasets/no_noise_dataset.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_tensor = torch.tensor(image_tensor, dtype=self.image_data_type)\n",
      "/home/francesco/Desktop/master_thesis_code/src/deep_learning/NN_datasets/custom_datasets/no_noise_dataset.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_tensor = torch.tensor(label_tensor, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Time taken to load 10 batches: 3.0651 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from deep_learning import custom_dataloader\n",
    "from deep_learning import NoNoiseDataset\n",
    "from deep_learning import ResNet50\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from shared_utils import _grid_lens\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "model = ResNet50(num_classes=2)\n",
    "model = model.float()  # Converts all weights and buffers to torch.float64\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Setup and Training ---\n",
    "grid_lens = _grid_lens(6.0, 100, device=device)\n",
    "\n",
    "\n",
    "\n",
    "# # Create an instance of your training dataset.\n",
    "# train_dataset=ResNetDataset(catalog_name=\"reposition_sources\", use_only_a_percent=50, mode=\"on_gpu_generation\", uncropped_grid=grid_lens)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=0)\n",
    "\n",
    "# Create dataset\n",
    "train_dataset= NoNoiseDataset(catalog_name=\"reposition_sources\", samples_used=2000, uncropped_grid=grid_lens, broadcasting=True)\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = custom_dataloader(train_dataset, batch_size=50, num_workers=0)\n",
    "\n",
    "import time\n",
    "time_start = time.time()\n",
    "# Debug: Check what one batch looks like\n",
    "for i in range(10):\n",
    "    test_images, test_labels = next(iter(train_loader))\n",
    "\n",
    "time_end = time.time()\n",
    "print(f\"\\n[DEBUG] Time taken to load 10 batches: {time_end - time_start:.4f} seconds\")\n",
    "\n",
    "\n",
    "train_dataset= NoNoiseDataset(catalog_name=\"reposition_sources\", samples_used=2000, uncropped_grid=grid_lens, broadcasting=False)\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = custom_dataloader(train_dataset, batch_size=50, num_workers=0)\n",
    "\n",
    "time_start = time.time()\n",
    "# Debug: Check what one batch looks like\n",
    "for i in range(10):\n",
    "    test_images, test_labels = next(iter(train_loader))\n",
    "\n",
    "time_end = time.time()\n",
    "print(f\"\\n[DEBUG] Time taken to load 10 batches: {time_end - time_start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Currently this dataloader is calculating the images in float32\n",
      "WARNING: The PEMD model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The External potential model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The PEMD model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The External potential model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The PEMD model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The External potential model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The PEMD model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The External potential model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The PEMD model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The External potential model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The PEMD model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The External potential model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The PEMD model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The External potential model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The PEMD model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The External potential model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The PEMD model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The External potential model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The PEMD model is not fully implemented yet or has to be tested in this batched version\n",
      "WARNING: The External potential model is not fully implemented yet or has to be tested in this batched version\n",
      "\n",
      "[DEBUG] Time taken to load 10 batches: 3.4370 seconds\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 100516 KiB |   1964 MiB |  86535 MiB |  86497 MiB |\n",
      "|       from large pool |  85937 KiB |   1950 MiB |  86533 MiB |  86495 MiB |\n",
      "|       from small pool |  14578 KiB |     14 MiB |      1 MiB |      1 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 100516 KiB |   1964 MiB |  86535 MiB |  86497 MiB |\n",
      "|       from large pool |  85937 KiB |   1950 MiB |  86533 MiB |  86495 MiB |\n",
      "|       from small pool |  14578 KiB |     14 MiB |      1 MiB |      1 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  86067 KiB |   1949 MiB |  86509 MiB |  86471 MiB |\n",
      "|       from large pool |  85937 KiB |   1949 MiB |  86509 MiB |  86471 MiB |\n",
      "|       from small pool |    130 KiB |      0 MiB |      0 MiB |      0 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   3008 MiB |   3008 MiB |   2816 MiB |  12288 KiB |\n",
      "|       from large pool |   2990 MiB |   2990 MiB |   2816 MiB |      0 KiB |\n",
      "|       from small pool |     18 MiB |     30 MiB |      0 MiB |  12288 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 139100 KiB |    824 MiB |  17160 MiB |  17156 MiB |\n",
      "|       from large pool | 135246 KiB |    821 MiB |  17159 MiB |  17155 MiB |\n",
      "|       from small pool |   3853 KiB |      3 MiB |      1 MiB |      1 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |   29160    |   29357    |    4900    |    4898    |\n",
      "|       from large pool |       3    |      66    |    1980    |    1979    |\n",
      "|       from small pool |   29157    |   29292    |    2920    |    2919    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |   29160    |   29357    |    4900    |    4898    |\n",
      "|       from large pool |       3    |      66    |    1980    |    1979    |\n",
      "|       from small pool |   29157    |   29292    |    2920    |    2919    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      43    |      43    |      32    |       6    |\n",
      "|       from large pool |      34    |      34    |      32    |       0    |\n",
      "|       from small pool |       9    |      15    |       0    |       6    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       8    |      30    |    2486    |    2484    |\n",
      "|       from large pool |       4    |      23    |    1533    |    1532    |\n",
      "|       from small pool |       4    |       8    |     953    |     952    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "Using device: cuda\n",
      "Currently this dataloader is calculating the images in float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/Desktop/master_thesis_code/src/deep_learning/NN_datasets/custom_datasets/no_noise_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_tensor = torch.tensor(num_sub > 0, device=self.device).long()\n",
      "/home/francesco/Desktop/master_thesis_code/src/deep_learning/NN_datasets/custom_datasets/no_noise_dataset.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_tensor = torch.tensor(image_tensor, dtype=self.image_data_type)\n",
      "/home/francesco/Desktop/master_thesis_code/src/deep_learning/NN_datasets/custom_datasets/no_noise_dataset.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_tensor = torch.tensor(label_tensor, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Time taken to load 10 batches: 3.6966 seconds\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 100516 KiB | 218708 KiB |  81765 MiB |  81765 MiB |\n",
      "|       from large pool |  85937 KiB | 204115 KiB |  81308 MiB |  81308 MiB |\n",
      "|       from small pool |  14578 KiB |  29156 KiB |    456 MiB |    456 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 100516 KiB | 218708 KiB |  81765 MiB |  81765 MiB |\n",
      "|       from large pool |  85937 KiB | 204115 KiB |  81308 MiB |  81308 MiB |\n",
      "|       from small pool |  14578 KiB |  29156 KiB |    456 MiB |    456 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  86067 KiB | 203255 KiB |  81355 MiB |  81355 MiB |\n",
      "|       from large pool |  85937 KiB | 203125 KiB |  80920 MiB |  80920 MiB |\n",
      "|       from small pool |    130 KiB |   4322 KiB |    435 MiB |    435 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 251904 KiB |   3008 MiB |  12288 KiB |   2774 MiB |\n",
      "|       from large pool | 221184 KiB |   2990 MiB |      0 KiB |   2774 MiB |\n",
      "|       from small pool |  30720 KiB |     30 MiB |  12288 KiB |      0 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  96092 KiB | 139358 KiB |  84918 MiB |  84960 MiB |\n",
      "|       from large pool |  92238 KiB | 135246 KiB |  84440 MiB |  84482 MiB |\n",
      "|       from small pool |   3853 KiB |   4920 KiB |    478 MiB |    478 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |   29160    |   58316    |   59046    |   59046    |\n",
      "|       from large pool |       3    |      24    |   13053    |   13053    |\n",
      "|       from small pool |   29157    |   58313    |   45993    |   45993    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |   29160    |   58316    |   59046    |   59046    |\n",
      "|       from large pool |       3    |      24    |   13053    |   13053    |\n",
      "|       from small pool |   29157    |   58313    |   45993    |   45993    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      18    |      43    |       6    |      31    |\n",
      "|       from large pool |       3    |      34    |       0    |      31    |\n",
      "|       from small pool |      15    |      15    |       6    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       7    |      19    |   21270    |   21271    |\n",
      "|       from large pool |       3    |       9    |    7077    |    7078    |\n",
      "|       from small pool |       4    |      12    |   14193    |   14193    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from deep_learning import custom_dataloader\n",
    "from deep_learning import NoNoiseDataset\n",
    "from deep_learning import ResNet50\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from shared_utils import _grid_lens\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Setup and Training ---\n",
    "grid_lens = _grid_lens(6.0, 1000,device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# # Create an instance of your training dataset.\n",
    "# train_dataset=ResNetDataset(catalog_name=\"reposition_sources\", use_only_a_percent=50, mode=\"on_gpu_generation\", uncropped_grid=grid_lens)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=0)\n",
    "\n",
    "# Create dataset\n",
    "train_dataset= NoNoiseDataset(catalog_name=\"first_bad_testing_catalog\", samples_used=1000, uncropped_grid=grid_lens, broadcasting=True)\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = custom_dataloader(train_dataset, batch_size=10, num_workers=0)\n",
    "\n",
    "import time\n",
    "time_start = time.time()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.reset_accumulated_memory_stats()  \n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "# Debug: Check what one batch looks like\n",
    "for i in range(10):\n",
    "    test_images, test_labels = next(iter(train_loader))\n",
    "\n",
    "time_end = time.time()\n",
    "print(f\"\\n[DEBUG] Time taken to load 10 batches: {time_end - time_start:.4f} seconds\")\n",
    "print(torch.cuda.memory_summary())\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.reset_accumulated_memory_stats()  \n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "train_dataset= NoNoiseDataset(catalog_name=\"first_bad_testing_catalog\", samples_used=1000, uncropped_grid=grid_lens, broadcasting=False)\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = custom_dataloader(train_dataset, batch_size=10, num_workers=0)\n",
    "\n",
    "time_start = time.time()\n",
    "# Debug: Check what one batch looks like\n",
    "for i in range(10):\n",
    "    test_images, test_labels = next(iter(train_loader))\n",
    "\n",
    "time_end = time.time()\n",
    "print(f\"\\n[DEBUG] Time taken to load 10 batches: {time_end - time_start:.4f} seconds\")\n",
    "print(torch.cuda.memory_summary())\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.reset_accumulated_memory_stats()  \n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NoNoiseDataset' from 'deep_learning' (/home/francesco/Desktop/master_thesis_code/src/deep_learning/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeep_learning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_dataloader\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeep_learning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NoNoiseDataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshared_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _grid_lens\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'NoNoiseDataset' from 'deep_learning' (/home/francesco/Desktop/master_thesis_code/src/deep_learning/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from deep_learning import custom_dataloader\n",
    "from deep_learning import NoNoiseDataset\n",
    "import torch.nn as nn\n",
    "from shared_utils import _grid_lens\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# --- Setup and Training ---\n",
    "grid_lens = _grid_lens(6.0, 1000,device=device)\n",
    "\n",
    "\n",
    "train_dataset= NoNoiseDataset(catalog_name=\"first_bad_testing_catalog\", samples_used=1000, uncropped_grid=grid_lens, broadcasting=True)\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = custom_dataloader(train_dataset, batch_size=1, num_workers=0)\n",
    "\n",
    "import time\n",
    "time_start = time.time()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.reset_accumulated_memory_stats()  \n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "def func():\n",
    "    for i in range(1):\n",
    "        test_images, test_labels = next(iter(train_loader))\n",
    "    \n",
    "    return test_images\n",
    "%prun images= func()\n",
    "\n",
    "time_end = time.time()\n",
    "print(f\"\\n[DEBUG] Time taken to load 10 batches: {time_end - time_start:.4f} seconds\")\n",
    "print(torch.cuda.memory_summary())\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.reset_accumulated_memory_stats()  \n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "plt.imshow(images[0].cpu().numpy(), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "[DEBUG] Time taken to load 10 batches: 2.7286 seconds\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 162116 KiB |   2965 MiB |  45188 MiB |  45188 MiB |\n",
      "|       from large pool | 125000 KiB |   2929 MiB |  45185 MiB |  45185 MiB |\n",
      "|       from small pool |  37116 KiB |     36 MiB |      2 MiB |      2 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 162116 KiB |   2965 MiB |  45188 MiB |  45188 MiB |\n",
      "|       from large pool | 125000 KiB |   2929 MiB |  45185 MiB |  45185 MiB |\n",
      "|       from small pool |  37116 KiB |     36 MiB |      2 MiB |      2 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 125329 KiB |   2922 MiB |  45084 MiB |  45084 MiB |\n",
      "|       from large pool | 125000 KiB |   2922 MiB |  45083 MiB |  45083 MiB |\n",
      "|       from small pool |    329 KiB |      0 MiB |      0 MiB |      0 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   3186 MiB |   3186 MiB |   2898 MiB |      0 B   |\n",
      "|       from large pool |   3148 MiB |   3148 MiB |   2898 MiB |      0 B   |\n",
      "|       from small pool |     38 MiB |     38 MiB |      0 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  30396 KiB |   1091 MiB |  30559 MiB |  30659 MiB |\n",
      "|       from large pool |  28600 KiB |   1090 MiB |  30557 MiB |  30657 MiB |\n",
      "|       from small pool |   1796 KiB |      1 MiB |      2 MiB |      2 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |   74234    |   74729    |    7730    |    7730    |\n",
      "|       from large pool |       2    |     170    |    2460    |    2460    |\n",
      "|       from small pool |   74232    |   74560    |    5270    |    5270    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |   74234    |   74729    |    7730    |    7730    |\n",
      "|       from large pool |       2    |     170    |    2460    |    2460    |\n",
      "|       from small pool |   74232    |   74560    |    5270    |    5270    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      49    |      49    |      28    |       0    |\n",
      "|       from large pool |      30    |      30    |      28    |       0    |\n",
      "|       from small pool |      19    |      19    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       3    |      35    |    1740    |    1740    |\n",
      "|       from large pool |       2    |      31    |    1244    |    1244    |\n",
      "|       from small pool |       1    |       5    |     496    |     496    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         121075 function calls (104715 primitive calls) in 2.722 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      770    1.060    0.001    1.060    0.001 {built-in method torch.tensor}\n",
      "     4040    0.372    0.000    0.372    0.000 {built-in method torch.empty}\n",
      "       10    0.167    0.017    0.718    0.072 lens_model.py:147(forward)\n",
      "     6246    0.090    0.000    0.090    0.000 {built-in method torch.as_tensor}\n",
      "       10    0.073    0.007    0.329    0.033 nfw.py:268(deflection_angle)\n",
      "      300    0.070    0.000    0.508    0.002 gaussian_blob.py:150(forward)\n",
      "       10    0.067    0.007    0.552    0.055 lens_model.py:118(deflection_field)\n",
      "     1500    0.061    0.000    0.061    0.000 {method 'copy_' of 'torch._C.TensorBase' objects}\n",
      "     1260    0.054    0.000    0.054    0.000 {built-in method torch.mul}\n",
      "      300    0.045    0.000    0.151    0.001 gaussian_blob.py:10(__init__)\n",
      "       20    0.045    0.002    0.045    0.002 {built-in method torch.clamp}\n",
      "     4650    0.033    0.000    0.068    0.000 module.py:1897(__setattr__)\n",
      "      620    0.033    0.000    0.033    0.000 {method 'pow' of 'torch._C.TensorBase' objects}\n",
      "       10    0.030    0.003    1.267    0.127 lensing_system.py:59(forward)\n",
      "       10    0.029    0.003    2.690    0.269 no_noise_dataset.py:32(get_batch)\n",
      "       20    0.023    0.001    0.023    0.001 {built-in method torch.where}\n",
      "       10    0.023    0.002    0.023    0.002 {built-in method torch.rsub}\n",
      "      600    0.022    0.000    0.022    0.000 {built-in method torch.exp}\n",
      "       10    0.021    0.002    0.021    0.002 {built-in method torch.zeros_like}\n",
      "      300    0.021    0.000    0.040    0.000 gaussian_blob.py:117(_get_rotation_factor)\n",
      "     1500    0.020    0.000    0.020    0.000 {method 'expand' of 'torch._C.TensorBase' objects}\n",
      "       10    0.019    0.002    0.019    0.002 {method 'repeat' of 'torch._C.TensorBase' objects}\n",
      " 8928/300    0.019    0.000    0.126    0.000 util.py:4(recursive_to_tensor)\n",
      "     1890    0.019    0.000    0.019    0.000 {method 'view' of 'torch._C.TensorBase' objects}\n",
      "      320    0.018    0.000    0.018    0.000 {built-in method torch.addcmul}\n",
      "      300    0.017    0.000    0.017    0.000 {built-in method torch.pow}\n",
      "       10    0.016    0.002    0.035    0.004 nfw.py:183(__init__)\n",
      "     4650    0.016    0.000    0.022    0.000 parameter.py:10(__instancecheck__)\n",
      "38440/33780    0.016    0.000    0.036    0.000 {built-in method builtins.isinstance}\n",
      "      320    0.016    0.000    0.016    0.000 {built-in method torch.sub}\n",
      "      340    0.015    0.000    0.015    0.000 {built-in method torch.div}\n",
      "      300    0.014    0.000    0.014    0.000 {method 'long' of 'torch._C.TensorBase' objects}\n",
      "      300    0.014    0.000    0.102    0.000 gaussian_blob.py:129(_optimized_translate_rotate)\n",
      "      300    0.011    0.000    0.011    0.000 {method 'zero_' of 'torch._C.TensorBase' objects}\n",
      " 2382/300    0.010    0.000    0.126    0.000 util.py:7(<dictcomp>)\n",
      "      630    0.009    0.000    0.010    0.000 {method 'squeeze' of 'torch._C.TensorBase' objects}\n",
      "      350    0.009    0.000    0.011    0.000 module.py:472(__init__)\n",
      "      300    0.008    0.000    0.009    0.000 gaussian_blob.py:204(__del__)\n",
      "       20    0.008    0.000    0.981    0.049 lens_model.py:73(build_param_tensor)\n",
      "      300    0.008    0.000    0.094    0.000 gaussian_blob.py:75(_ensure_buffers)\n",
      "     1820    0.008    0.000    0.008    0.000 {method 'to' of 'torch._C.TensorBase' objects}\n",
      "       10    0.006    0.001    0.018    0.002 sis.py:9(__init__)\n",
      "      300    0.006    0.000    0.006    0.000 {built-in method torch.view_as_complex}\n",
      "       10    0.005    0.001    2.704    0.270 dataloaders.py:48(<lambda>)\n",
      "      300    0.005    0.000    0.005    0.000 {built-in method torch.view_as_real}\n",
      "    14230    0.005    0.000    0.005    0.000 {method 'get' of 'dict' objects}\n",
      "       10    0.004    0.000    1.334    0.133 lensing_system.py:13(__init__)\n",
      "       10    0.003    0.000    1.042    0.104 lens_model.py:38(process_configs)\n",
      "      630    0.003    0.000    0.059    0.000 _tensor.py:33(wrapped)\n",
      "       20    0.003    0.000    0.007    0.000 samplers.py:14(__iter__)\n",
      "   320/10    0.003    0.000    1.267    0.127 module.py:1718(_call_impl)\n",
      "     4650    0.003    0.000    0.003    0.000 {function _ParameterMeta.__instancecheck__ at 0xfffec814fc70}\n",
      "      620    0.002    0.000    0.002    0.000 module.py:1880(__getattr__)\n",
      "        1    0.002    0.002    2.722    2.722 3405835740.py:8(func)\n",
      "       40    0.002    0.000    0.002    0.000 {built-in method torch.sqrt}\n",
      "       20    0.002    0.000    0.002    0.000 {method 'random_' of 'torch._C.TensorBase' objects}\n",
      "      350    0.002    0.000    0.002    0.000 {built-in method torch._C._log_api_usage_once}\n",
      "       10    0.002    0.000    0.002    0.000 {method 'tolist' of 'torch._C.TensorBase' objects}\n",
      "      300    0.002    0.000    0.005    0.000 module.py:630(add_module)\n",
      "       10    0.002    0.000    0.002    0.000 {built-in method torch.stack}\n",
      "       10    0.002    0.000    0.002    0.000 {built-in method torch.randperm}\n",
      "       10    0.002    0.000    0.002    0.000 {built-in method torch._ops.profiler._record_function_enter_new}\n",
      "     3160    0.001    0.000    0.001    0.000 {built-in method builtins.getattr}\n",
      "       20    0.001    0.000    0.001    0.000 {method 'index_add_' of 'torch._C.TensorBase' objects}\n",
      "      320    0.001    0.000    0.003    0.000 {built-in method builtins.hasattr}\n",
      "   320/10    0.001    0.000    1.267    0.127 module.py:1710(_wrapped_call_impl)\n",
      "      320    0.001    0.000    0.001    0.000 {built-in method torch._C._get_tracing_state}\n",
      "       10    0.001    0.000    0.133    0.013 sis.py:65(deflection_angle)\n",
      "       20    0.001    0.000    0.001    0.000 {built-in method torch.maximum}\n",
      "       20    0.001    0.000    0.001    0.000 {built-in method torch.log}\n",
      "       10    0.001    0.000    0.001    0.000 {built-in method torch.zeros}\n",
      "      300    0.001    0.000    0.006    0.000 container.py:407(append)\n",
      "      630    0.001    0.000    0.001    0.000 {built-in method torch._C._has_torch_function}\n",
      "      300    0.001    0.000    0.001    0.000 {method 'contiguous' of 'torch._C.TensorBase' objects}\n",
      "       10    0.001    0.000    0.001    0.000 {built-in method torch.ones_like}\n",
      "      300    0.001    0.000    0.010    0.000 source_model.py:8(__init__)\n",
      "     1923    0.001    0.000    0.001    0.000 {method 'append' of 'list' objects}\n",
      "      300    0.001    0.000    0.001    0.000 {method 'clear' of 'dict' objects}\n",
      "     2402    0.001    0.000    0.001    0.000 {method 'items' of 'dict' objects}\n",
      "       10    0.001    0.000    0.001    0.000 {built-in method torch.atan}\n",
      "      300    0.001    0.000    0.064    0.000 util.py:16(<listcomp>)\n",
      "       10    0.001    0.000    0.001    0.000 {built-in method torch.atanh}\n",
      "  720/420    0.001    0.000    0.001    0.000 {built-in method builtins.len}\n",
      "       10    0.001    0.000    0.003    0.000 dataloader.py:625(__init__)\n",
      "       10    0.001    0.000    0.001    0.000 {built-in method torch.add}\n",
      "      600    0.001    0.000    0.001    0.000 util.py:12(<genexpr>)\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method torch._ops.profiler.}\n",
      "       10    0.000    0.000    0.127    0.013 lensing_system.py:28(<listcomp>)\n",
      "       40    0.000    0.000    0.001    0.000 ipkernel.py:775(_clean_thread_parent_frames)\n",
      "       10    0.000    0.000    0.155    0.015 nfw.py:238(_initialize_buffers)\n",
      "       20    0.000    0.000    0.000    0.000 {method 'item' of 'torch._C.TensorBase' objects}\n",
      "       10    0.000    0.000    2.716    0.272 dataloader.py:696(__next__)\n",
      "       10    0.000    0.000    0.002    0.000 profiler.py:694(__exit__)\n",
      "      300    0.000    0.000    0.000    0.000 container.py:341(__len__)\n",
      "       20    0.000    0.000    0.000    0.000 threading.py:1478(enumerate)\n",
      "       10    0.000    0.000    0.126    0.013 sis.py:44(_initialize_buffers)\n",
      "    70/20    0.000    0.000    0.001    0.000 _pytree.py:874(tree_iter)\n",
      "       10    0.000    0.000    1.043    0.104 lens_model.py:11(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 {method 'unsqueeze' of 'torch._C.TensorBase' objects}\n",
      "      140    0.000    0.000    0.000    0.000 threading.py:1145(ident)\n",
      "       20    0.000    0.000    0.000    0.000 ipkernel.py:790(<setcomp>)\n",
      "       10    0.000    0.000    0.000    0.000 lensing_system.py:57(<listcomp>)\n",
      "      300    0.000    0.000    0.001    0.000 {built-in method builtins.all}\n",
      "       10    0.000    0.000    0.000    0.000 fetch.py:52(<listcomp>)\n",
      "      320    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}\n",
      "       10    0.000    0.000    0.004    0.000 dataloader.py:734(__init__)\n",
      "       70    0.000    0.000    0.000    0.000 _pytree.py:638(_is_namedtuple_instance)\n",
      "      300    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
      "       10    0.000    0.000    0.000    0.000 profiler.py:677(__init__)\n",
      "       30    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "       10    0.000    0.000    2.705    0.270 fetch.py:47(fetch)\n",
      "       20    0.000    0.000    0.001    0.000 mass_component.py:5(__init__)\n",
      "       10    0.000    0.000    2.712    0.271 dataloader.py:755(_next_data)\n",
      "       10    0.000    0.000    0.002    0.000 profiler.py:688(__enter__)\n",
      "       10    0.000    0.000    0.000    0.000 dataloader.py:74(create_fetcher)\n",
      "        1    0.000    0.000    2.722    2.722 {built-in method builtins.exec}\n",
      "       10    0.000    0.000    0.000    0.000 lensing_system.py:31(<listcomp>)\n",
      "       10    0.000    0.000    0.002    0.000 _ops.py:890(__call__)\n",
      "       10    0.000    0.000    0.001    0.000 _pytree.py:1181(tree_any)\n",
      "       10    0.000    0.000    0.023    0.002 _tensor.py:967(__rsub__)\n",
      "    30/20    0.000    0.000    0.004    0.000 {built-in method builtins.iter}\n",
      "       40    0.000    0.000    0.000    0.000 _pytree.py:656(_is_leaf)\n",
      "      300    0.000    0.000    0.000    0.000 {method 'clear' of 'set' objects}\n",
      "       10    0.000    0.000    0.001    0.000 _ops.py:946(_must_dispatch_in_python)\n",
      "       70    0.000    0.000    0.000    0.000 _pytree.py:649(_get_node_type)\n",
      "       10    0.000    0.000    0.004    0.000 dataloader.py:410(_get_iterator)\n",
      "    20/10    0.000    0.000    2.716    0.272 {built-in method builtins.next}\n",
      "       10    0.000    0.000    0.000    0.000 _ops.py:948(<lambda>)\n",
      "       10    0.000    0.000    0.000    0.000 typing.py:306(inner)\n",
      "       10    0.000    0.000    0.000    0.000 {method 'manual_seed' of 'torch._C.Generator' objects}\n",
      "      300    0.000    0.000    0.000    0.000 dataloaders.py:39(__getitem__)\n",
      "       10    0.000    0.000    0.002    0.000 _ops.py:1050(__call__)\n",
      "       10    0.000    0.000    0.000    0.000 container.py:345(__iter__)\n",
      "       10    0.000    0.000    0.000    0.000 _pytree.py:423(_dict_flatten)\n",
      "       10    0.000    0.000    0.004    0.000 dataloader.py:471(__iter__)\n",
      "       10    0.000    0.000    0.000    0.000 dataloader.py:97(_get_distributed_settings)\n",
      "       30    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
      "       20    0.000    0.000    0.000    0.000 module.py:1898(remove_from)\n",
      "       60    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "       10    0.000    0.000    0.001    0.000 {built-in method builtins.any}\n",
      "       10    0.000    0.000    0.000    0.000 container.py:306(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 lensing_system.py:32(<listcomp>)\n",
      "       10    0.000    0.000    0.000    0.000 fetch.py:9(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "       90    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "       20    0.000    0.000    0.000    0.000 _pytree.py:395(_tuple_flatten)\n",
      "       10    0.000    0.000    0.000    0.000 __init__.py:8(is_available)\n",
      "       10    0.000    0.000    0.007    0.001 dataloader.py:690(_next_index)\n",
      "       20    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "       20    0.000    0.000    0.000    0.000 dataloader.py:486(_auto_collation)\n",
      "       10    0.000    0.000    0.000    0.000 dataloader.py:490(_index_sampler)\n",
      "        1    0.000    0.000    2.722    2.722 <string>:1(<module>)\n",
      "       10    0.000    0.000    0.000    0.000 {method '__exit__' of 'torch._C.DisableTorchFunctionSubclass' objects}\n",
      "       10    0.000    0.000    0.000    0.000 _jit_internal.py:1135(is_scripting)\n",
      "       10    0.000    0.000    0.000    0.000 __init__.py:129(annotate)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time_start = time.time()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.reset_accumulated_memory_stats()  \n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "def func():\n",
    "    for i in range(10):\n",
    "        test_images, test_labels = next(iter(train_loader))\n",
    "    \n",
    "    return test_images\n",
    "%prun images= func()\n",
    "\n",
    "time_end = time.time()\n",
    "print(f\"\\n[DEBUG] Time taken to load 10 batches: {time_end - time_start:.4f} seconds\")\n",
    "print(torch.cuda.memory_summary())\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.reset_accumulated_memory_stats()  \n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "plt.imshow(images[0].cpu().numpy(), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Currently this dataloader is calculating the images in float32\n",
      "Using device: cuda\n",
      "Currently this dataloader is calculating the images in float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/Desktop/master_thesis_code/src/deep_learning/NN_datasets/custom_datasets/no_noise_dataset.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_tensor = torch.tensor(num_sub > 0, device=self.device).long()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from deep_learning import custom_dataloader\n",
    "from deep_learning import NoNoiseDataset\n",
    "from matplotlib import pyplot as plt\n",
    "from shared_utils import _grid_lens\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# --- Setup and Training ---\n",
    "grid_lens = _grid_lens(6.0, 1000,device=device)\n",
    "\n",
    "\n",
    "# Create datasets with broadcasting=True and broadcasting=False\n",
    "dataset_broadcasting_true = NoNoiseDataset(catalog_name=\"first_bad_testing_catalog\", samples_used=1000, uncropped_grid=grid_lens, broadcasting=True)\n",
    "dataset_broadcasting_false = NoNoiseDataset(catalog_name=\"first_bad_testing_catalog\", samples_used=1000, uncropped_grid=grid_lens, broadcasting=False)\n",
    "\n",
    "# Create dataloaders\n",
    "loader_broadcasting_true = custom_dataloader(dataset_broadcasting_true, batch_size=30, num_workers=0, shuffle=False)\n",
    "loader_broadcasting_false = custom_dataloader(dataset_broadcasting_false, batch_size=30, num_workers=0, shuffle=False)\n",
    "\n",
    "# Get one batch from each loader\n",
    "images_true, _ = next(iter(loader_broadcasting_true))\n",
    "images_false, _ = next(iter(loader_broadcasting_false))\n",
    "\n",
    "# Plot the first image from each batch for comparison\n",
    "for i in range(30):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Broadcasting=True\")\n",
    "    plt.imshow(images_true[i].cpu().numpy().squeeze(), cmap='viridis')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Broadcasting=False\")\n",
    "    plt.imshow(images_false[i].cpu().numpy().squeeze(), cmap='viridis')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My HPC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
